{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from datetime import datetime\n",
    "\n",
    "def signature():\n",
    "    display(Markdown(f\"\"\"\n",
    "---\n",
    "### 👨‍💻 *Authored by [Erwin R. Pasia](mailto:erwinpasia@gmail.com)*\n",
    "📅 Date: {datetime.now().strftime(\"%B %d, %Y\")}\n",
    "\n",
    "> *\"Code is poetry. Simplicity is elegance.\"*  \n",
    "---\n",
    "\"\"\"))\n",
    "\n",
    "signature()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## Capstone Project Title: \"CafeGenius\" Re-Architected with LangGraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About: \n",
    "-  The notebook presents \"CafeGenius,\" a conversational AI assistant designed for a cafe environment. Its primary functions are to help customers explore the menu, get personalized recommendations, inquire about specific items, build an order incrementally across multiple conversation turns, modify that order (add, remove, clear), confirm the final order, and simulate placing it.\n",
    "\n",
    "- The use case is highly relatable and addresses a common interaction scenario. The ability to handle multi-turn conversations, maintain order state, and ground responses in actual menu data (via RAG) makes it significantly more useful than a basic FAQ bot.\n",
    "\n",
    "- While cafe ordering bots aren't entirely new, the approach taken here demonstrates innovation. Specifically, the deliberate re-architecture from a simpler script (implied context) to a stateful agent using LangGraph showcases a modern, robust way to build such applications. The tight integration of RAG (Gemini Embeddings + ChromaDB) for accurate information retrieval and Gemini Function Calling for structured actions within the LangGraph framework is a sophisticated implementation.\n",
    "\n",
    "- Within the context of the Capstone Project, it's impactful as it demonstrates a practical application of multiple key GenAI concepts (LLMs, RAG, Function Calling, Agentic Frameworks) to solve a tangible problem. It serves as an excellent learning example for building reliable, stateful conversational AI.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 (Setup and Installation):\n",
    "- This section ensures the necessary software foundation is in place by installing all required Python libraries using %pip install. It covers core dependencies like LangGraph, LangChain components (langchain-google-genai, langchain-core), the vector database (chromadb), Google API clients, and utilities for handling secrets (kaggle, python-dotenv) and running within a Jupyter environment (ipython, ipykernel), concluding with a crucial reminder to restart the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:46:11.599725Z",
     "iopub.status.busy": "2025-04-12T23:46:11.599516Z",
     "iopub.status.idle": "2025-04-12T23:46:54.709811Z",
     "shell.execute_reply": "2025-04-12T23:46:54.709086Z",
     "shell.execute_reply.started": "2025-04-12T23:46:11.599702Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.8/210.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m604.1/604.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\n",
      "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.27.0, but you have google-auth 2.38.0 which is incompatible.\n",
      "google-colab 1.0.0 requires ipykernel==6.17.1, but you have ipykernel 6.29.5 which is incompatible.\n",
      "google-colab 1.0.0 requires ipython==7.34.0, but you have ipython 9.1.0 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "google-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\n",
      "pandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.2 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.2 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\n",
      "ibis-framework 9.2.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "ibis-framework 9.2.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\n",
      "google-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Package installation/update complete.\n"
     ]
    }
   ],
   "source": [
    "### 1. Setup and Installation\n",
    "#\n",
    "# Install necessary packages, including LangGraph, LangChain components, ChromaDB, and Google Generative AI.\n",
    "\n",
    "# Remove potentially conflicting packages (optional, adjust if needed)\n",
    "# %pip uninstall -qqy kfp jupyterlab\n",
    "\n",
    "# Install required packages\n",
    "# Using versions known to be relatively stable at the time of writing (example)\n",
    "%pip install --upgrade --quiet \"langgraph>=0.0.50\" \"langchain-google-genai>=1.0.3\" \"langchain-core>=0.1.40\" \"chromadb>=0.5.0\" \"google-api-python-client\" \"google-auth\" \"kaggle\" \"ipython\" \"ipykernel\" \"typing_extensions\" \"python-dotenv\"\n",
    "\n",
    "print(\"Package installation/update complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 (Imports):\n",
    "- This cell imports all necessary modules, classes, and functions from the installed libraries and Python's standard library, making them accessible for use throughout the notebook. It organizes imports logically, bringing in components for LangGraph state/graph management, LangChain messages and tools, the Google Generative AI chat model, ChromaDB functionalities, standard utilities like os and logging, and helpers for display and secret management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:46:54.711155Z",
     "iopub.status.busy": "2025-04-12T23:46:54.710961Z",
     "iopub.status.idle": "2025-04-12T23:46:57.780595Z",
     "shell.execute_reply": "2025-04-12T23:46:57.779904Z",
     "shell.execute_reply.started": "2025-04-12T23:46:54.711139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete.\n"
     ]
    }
   ],
   "source": [
    "### 2. Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from random import randint\n",
    "from typing import List, Dict, Any, Optional, Union, Annotated, Literal\n",
    "\n",
    "# LangGraph & LangChain\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import AIMessage, ToolMessage, HumanMessage, BaseMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Google Generative AI (for Embeddings mainly now)\n",
    "import google.generativeai as genai\n",
    "from google.api_core import retry\n",
    "from google.api_core import exceptions as api_core_exceptions\n",
    "\n",
    "# ChromaDB\n",
    "import chromadb\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "# Other Utilities\n",
    "from IPython.display import Markdown, display, Image\n",
    "# Use Kaggle secrets or dotenv for local development\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    _use_kaggle_secrets = True\n",
    "except ImportError:\n",
    "    # Fallback for local dev: install python-dotenv (`pip install python-dotenv`)\n",
    "    # and create a .env file with GOOGLE_API_KEY=...\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    _use_kaggle_secrets = False\n",
    "    print(\"Kaggle secrets not found, attempting to load from .env file for local development.\")\n",
    "\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 (Logging Configuration):\n",
    "- This section configures Python's built-in logging module to provide runtime insights into the agent's operation. It sets up a basic configuration to display INFO level messages (and above) with timestamps and module names, creating a dedicated logger instance (cafe_genius_langgraph) to help track execution flow and diagnose potential issues during development and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:46:57.781620Z",
     "iopub.status.busy": "2025-04-12T23:46:57.781398Z",
     "iopub.status.idle": "2025-04-12T23:46:57.785528Z",
     "shell.execute_reply": "2025-04-12T23:46:57.784637Z",
     "shell.execute_reply.started": "2025-04-12T23:46:57.781603Z"
    }
   },
   "outputs": [],
   "source": [
    "### 3. Logging Configuration\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger('cafe_genius_langgraph')\n",
    "logger.info(\"Logging setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4 (API Key Configuration):\n",
    "- This section securely retrieves the Google API key required for accessing Gemini models, handling different environments by first trying Kaggle secrets and falling back to environment variables (via .env file and python-dotenv). It then configures the genai library (for embeddings) and sets the key as an environment variable for LangChain, ensuring proper authentication for all subsequent Google AI service calls while including error handling for missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:46:57.787158Z",
     "iopub.status.busy": "2025-04-12T23:46:57.786972Z",
     "iopub.status.idle": "2025-04-12T23:46:58.014371Z",
     "shell.execute_reply": "2025-04-12T23:46:58.013737Z",
     "shell.execute_reply.started": "2025-04-12T23:46:57.787141Z"
    }
   },
   "outputs": [],
   "source": [
    "### 4. API Key Configuration\n",
    "\n",
    "try:\n",
    "    if _use_kaggle_secrets:\n",
    "        GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "        logger.info(\"Using GOOGLE_API_KEY from Kaggle secrets.\")\n",
    "    else:\n",
    "        # Load from environment variable (e.g., from .env file)\n",
    "        GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "        if GOOGLE_API_KEY:\n",
    "            logger.info(\"Using GOOGLE_API_KEY from environment variable.\")\n",
    "\n",
    "    if not GOOGLE_API_KEY:\n",
    "        logger.error(\"API key not found. Set GOOGLE_API_KEY Kaggle secret or in .env file.\")\n",
    "        print(\"ERROR: GOOGLE_API_KEY not found.\")\n",
    "    else:\n",
    "        # Configure the GenAI client (still needed for embeddings)\n",
    "        genai.configure(api_key=GOOGLE_API_KEY)\n",
    "        # LangChain will automatically pick up the key from the environment variable\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "        logger.info(\"Google Generative AI client configured (for embeddings) and environment variable set for LangChain.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error configuring API key: {e}\")\n",
    "    print(f\"Error configuring API key: {e}\")\n",
    "    GOOGLE_API_KEY = None # Ensure it's None if setup fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5 (Gemini Embedding Function for ChromaDB):\n",
    "- This section defines a custom GeminiEmbeddingFunction class that integrates Google's text-embedding-004 model with ChromaDB. It implements ChromaDB's required interface, handling the API calls to genai.embed_content, managing different embedding task types (retrieval_document vs. retrieval_query), processing the API response to extract vectors, and incorporating robust error handling and automatic retries for API stability, thus enabling semantic vectorization of menu data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:46:58.015243Z",
     "iopub.status.busy": "2025-04-12T23:46:58.015042Z",
     "iopub.status.idle": "2025-04-12T23:46:58.023379Z",
     "shell.execute_reply": "2025-04-12T23:46:58.022294Z",
     "shell.execute_reply.started": "2025-04-12T23:46:58.015225Z"
    }
   },
   "outputs": [],
   "source": [
    "### 5. Gemini Embedding Function for ChromaDB\n",
    "#\n",
    "# This remains largely the same as the original, providing embeddings for the RAG component.\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "    \"\"\"Custom embedding function using Gemini API (text-embedding-004)\"\"\"\n",
    "    def __init__(self, api_key: Optional[str] = None, task_type=\"retrieval_document\", model_name=\"models/text-embedding-004\"):\n",
    "        if api_key:\n",
    "             genai.configure(api_key=api_key)\n",
    "        self.task_type = task_type\n",
    "        self.model_name = model_name\n",
    "        logger.info(f\"GeminiEmbeddingFunction initialized with model: {self.model_name}\")\n",
    "\n",
    "    @retry.Retry(predicate=retry.if_exception_type(\n",
    "        api_core_exceptions.Aborted,\n",
    "        api_core_exceptions.DeadlineExceeded,\n",
    "        api_core_exceptions.ServiceUnavailable,\n",
    "        api_core_exceptions.InternalServerError,\n",
    "        ConnectionError\n",
    "    ))\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        \"\"\"Embeds a list of documents.\"\"\"\n",
    "        if not input:\n",
    "            return []\n",
    "        current_task_type = self.task_type\n",
    "        logger.debug(f\"Embedding {len(input)} documents with task type: {current_task_type}\")\n",
    "        try:\n",
    "            if not isinstance(input, list) or not all(isinstance(doc, str) for doc in input):\n",
    "                raise TypeError(\"Input must be a list of strings (Documents).\")\n",
    "\n",
    "            response = genai.embed_content(\n",
    "                model=self.model_name,\n",
    "                content=input,\n",
    "                task_type=current_task_type\n",
    "            )\n",
    "            if 'embedding' in response and isinstance(response['embedding'], list):\n",
    "                 embeddings = response['embedding']\n",
    "                 # Handle potential API difference: single embedding might not be nested\n",
    "                 if embeddings and not isinstance(embeddings[0], list) and len(input) == 1:\n",
    "                      logger.debug(f\"Successfully embedded {len(input)} document (single list).\")\n",
    "                      return [embeddings] # Wrap single embedding in a list\n",
    "                 else:\n",
    "                      logger.debug(f\"Successfully embedded {len(embeddings)} documents.\")\n",
    "                      return embeddings\n",
    "            else:\n",
    "                 logger.error(f\"Unexpected embedding response format: {response}\")\n",
    "                 raise ValueError(\"Failed to extract embeddings from response.\")\n",
    "        except TypeError as te:\n",
    "             logger.error(f\"Input type error during embedding: {te}\")\n",
    "             raise\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error during embedding call: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6 (Menu Data Definition):\n",
    "- This section establishes the cafe's knowledge base by defining the menu_data as a list of dictionaries detailing each item (name, price, description, modifiers, availability). Crucially, it processes this raw data to create menu_descriptions, a list of formatted strings specifically designed for effective embedding and subsequent retrieval by the RAG system, providing the content for the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:46:58.024960Z",
     "iopub.status.busy": "2025-04-12T23:46:58.024659Z",
     "iopub.status.idle": "2025-04-12T23:46:58.046137Z",
     "shell.execute_reply": "2025-04-12T23:46:58.045523Z",
     "shell.execute_reply.started": "2025-04-12T23:46:58.024937Z"
    }
   },
   "outputs": [],
   "source": [
    "### 6. Menu Data Definition\n",
    "#\n",
    "# Same menu data as the original notebook.\n",
    "\n",
    "menu_data = [\n",
    "    # ... (Copy the full menu_data list from the original notebook here) ...\n",
    "    # Coffee\n",
    "    {\"name\": \"Espresso\", \"category\": \"Coffee\", \"description\": \"A concentrated form of coffee served in small, strong shots.\", \"price\": 3.50, \"available\": True, \"modifiers\": [\"Single\", \"Double\", \"Triple\", \"Decaf\", \"Regular\"]},\n",
    "    {\"name\": \"Americano\", \"category\": \"Coffee\", \"description\": \"Espresso diluted with hot water for a smoother taste.\", \"price\": 3.75, \"available\": True, \"modifiers\": [\"Hot\", \"Iced\", \"Decaf\"]},\n",
    "    {\"name\": \"Latte\", \"category\": \"Coffee\", \"description\": \"Espresso with steamed milk and a light layer of foam.\", \"price\": 4.50, \"available\": True, \"modifiers\": [\"Whole Milk\", \"2%\", \"Skim\", \"Oat Milk\", \"Almond\", \"Vanilla\", \"Caramel\", \"Hazelnut\"]},\n",
    "    {\"name\": \"Cappuccino\", \"category\": \"Coffee\", \"description\": \"Equal parts espresso, steamed milk, and foam.\", \"price\": 4.25, \"available\": True, \"modifiers\": [\"Whole Milk\", \"2%\", \"Oat Milk\", \"Almond\", \"Cinnamon\", \"Chocolate\"]},\n",
    "    {\"name\": \"Flat White\", \"category\": \"Coffee\", \"description\": \"Ristretto shots topped with steamed milk and a velvety microfoam.\", \"price\": 4.25, \"available\": True, \"modifiers\": [\"Whole Milk\", \"Oat Milk\", \"Almond\"]},\n",
    "    {\"name\": \"Cold Brew\", \"category\": \"Coffee\", \"description\": \"Coffee brewed with cold water over an extended period.\", \"price\": 4.75, \"available\": True, \"modifiers\": [\"Sweet Cream\", \"Vanilla\", \"Caramel\", \"Oat Milk\"]},\n",
    "    {\"name\": \"Mocha\", \"category\": \"Coffee\", \"description\": \"Espresso with chocolate syrup and steamed milk, topped with whipped cream.\", \"price\": 4.95, \"available\": True, \"modifiers\": [\"Dark Chocolate\", \"White Chocolate\", \"Oat Milk\", \"Almond Milk\", \"No Whip\"]},\n",
    "\n",
    "    # Tea\n",
    "    {\"name\": \"Chai Latte\", \"category\": \"Tea\", \"description\": \"Spiced tea concentrate with steamed milk.\", \"price\": 4.50, \"available\": True, \"modifiers\": [\"Whole Milk\", \"2%\", \"Oat Milk\", \"Almond\", \"Extra Spicy\"]},\n",
    "    {\"name\": \"Matcha Latte\", \"category\": \"Tea\", \"description\": \"Japanese green tea powder with steamed milk.\", \"price\": 5.00, \"available\": True, \"modifiers\": [\"Whole Milk\", \"2%\", \"Oat Milk\", \"Almond\", \"Vanilla\"]},\n",
    "    {\"name\": \"Earl Grey\", \"category\": \"Tea\", \"description\": \"Classic black tea infused with bergamot citrus.\", \"price\": 3.25, \"available\": True, \"modifiers\": [\"Honey\", \"Lemon\", \"Milk\"]},\n",
    "    {\"name\": \"Herbal Tea\", \"category\": \"Tea\", \"description\": \"Caffeine-free blend of herbs and botanicals.\", \"price\": 3.00, \"available\": True, \"modifiers\": [\"Peppermint\", \"Chamomile\", \"Lemon Ginger\"]},\n",
    "    {\"name\": \"Iced Tea\", \"category\": \"Tea\", \"description\": \"Refreshing black tea served over ice.\", \"price\": 3.50, \"available\": True, \"modifiers\": [\"Sweetened\", \"Unsweetened\", \"Lemon\", \"Peach\", \"Raspberry\"]},\n",
    "\n",
    "    # Pastries\n",
    "    {\"name\": \"Croissant\", \"category\": \"Pastry\", \"description\": \"Buttery, flaky pastry of French origin.\", \"price\": 3.25, \"available\": True, \"modifiers\": [\"Butter\", \"Almond\", \"Chocolate\"]},\n",
    "    {\"name\": \"Blueberry Muffin\", \"category\": \"Pastry\", \"description\": \"Sweet breakfast bread with blueberries.\", \"price\": 3.50, \"available\": True, \"modifiers\": [\"Warmed\"]},\n",
    "    {\"name\": \"Banana Bread\", \"category\": \"Pastry\", \"description\": \"Moist banana bread with a hint of cinnamon.\", \"price\": 3.75, \"available\": True, \"modifiers\": [\"Warmed\", \"Add Butter\"]},\n",
    "    {\"name\": \"Cinnamon Roll\", \"category\": \"Pastry\", \"description\": \"Swirled pastry with cinnamon and icing.\", \"price\": 4.00, \"available\": True, \"modifiers\": [\"Extra Icing\", \"Warmed\"]},\n",
    "\n",
    "    # Food\n",
    "    {\"name\": \"Avocado Toast\", \"category\": \"Food\", \"description\": \"Toasted bread topped with mashed avocado.\", \"price\": 7.50, \"available\": True, \"modifiers\": [\"Add Egg\", \"Add Tomato\", \"Add Feta\"]},\n",
    "    {\"name\": \"Breakfast Sandwich\", \"category\": \"Food\", \"description\": \"Egg and cheese on a croissant or English muffin.\", \"price\": 6.50, \"available\": True, \"modifiers\": [\"Bacon\", \"Sausage\", \"Avocado\"]},\n",
    "    {\"name\": \"Quiche\", \"category\": \"Food\", \"description\": \"Savory egg tart with cheese and seasonal vegetables.\", \"price\": 6.75, \"available\": True, \"modifiers\": [\"Vegetarian\", \"Add Ham\", \"Gluten-Free\"]},\n",
    "    {\"name\": \"Yogurt Parfait\", \"category\": \"Food\", \"description\": \"Layers of Greek yogurt, granola, and seasonal fruit.\", \"price\": 5.50, \"available\": True, \"modifiers\": [\"Honey\", \"No Granola\", \"Add Chia Seeds\"]},\n",
    "]\n",
    "\n",
    "logger.info(f\"Loaded {len(menu_data)} menu items.\")\n",
    "\n",
    "# Create detailed descriptions for RAG\n",
    "menu_descriptions = []\n",
    "for item in menu_data:\n",
    "    description = f\"{item['name']}: {item['description']} Priced at ${item['price']:.2f}.\"\n",
    "    if item.get('modifiers'):\n",
    "        description += f\" Available modifiers: {', '.join(item['modifiers'])}.\"\n",
    "    menu_descriptions.append(description)\n",
    "\n",
    "logger.info(f\"Generated {len(menu_descriptions)} descriptions for RAG.\")\n",
    "# print(\"Sample menu descriptions for RAG:\")\n",
    "# for i in range(min(3, len(menu_descriptions))): print(f\"- {menu_descriptions[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 7 (ChromaDB Setup for RAG):\n",
    "- This section initializes and prepares the ChromaDB vector database for Retrieval-Augmented Generation (RAG). It creates a persistent client (chromadb.PersistentClient), sets up a specific collection using the custom GeminiEmbeddingFunction, and populates this collection with the embedded menu_descriptions (along with metadata and unique IDs) only if the collection is initially empty, ensuring the menu data is vectorized and ready for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:46:58.046867Z",
     "iopub.status.busy": "2025-04-12T23:46:58.046673Z",
     "iopub.status.idle": "2025-04-12T23:47:00.064980Z",
     "shell.execute_reply": "2025-04-12T23:47:00.064327Z",
     "shell.execute_reply.started": "2025-04-12T23:46:58.046852Z"
    }
   },
   "outputs": [],
   "source": [
    "### 7. ChromaDB Setup for RAG\n",
    "#\n",
    "# Setup the vector database for menu searching.\n",
    "\n",
    "DB_PATH = \"./chroma_db_cafe_lg\" # Use a specific path for persistent storage\n",
    "COLLECTION_NAME = \"cafegenius_menu_lg_coll\"\n",
    "\n",
    "# Ensure API key is available before instantiating embedding function\n",
    "if GOOGLE_API_KEY:\n",
    "    embed_fn = GeminiEmbeddingFunction(api_key=GOOGLE_API_KEY)\n",
    "    try:\n",
    "        chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
    "        db = chroma_client.get_or_create_collection(\n",
    "            name=COLLECTION_NAME,\n",
    "            embedding_function=embed_fn,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        logger.info(f\"ChromaDB collection '{COLLECTION_NAME}' created or retrieved from path '{DB_PATH}'.\")\n",
    "\n",
    "        # Add menu descriptions if collection is empty\n",
    "        if db.count() == 0:\n",
    "             logger.info(f\"Adding {len(menu_descriptions)} documents to ChromaDB...\")\n",
    "             db.add(\n",
    "                 documents=menu_descriptions,\n",
    "                 ids=[item['name'] for item in menu_data], # Use item names as unique IDs\n",
    "                 metadatas=[{\"category\": item[\"category\"], \"price\": item[\"price\"]} for item in menu_data] # Add metadata\n",
    "             )\n",
    "             logger.info(\"Documents added successfully.\")\n",
    "             # Short delay might help ensure persistence completes before next step in some environments\n",
    "             import time\n",
    "             time.sleep(1)\n",
    "        else:\n",
    "             logger.info(f\"Collection '{COLLECTION_NAME}' already contains {db.count()} documents. Skipping add.\")\n",
    "\n",
    "        # Verification\n",
    "        if db.count() > 0:\n",
    "            sample_ids = [item['name'] for item in menu_data[:min(3, len(menu_data))]]\n",
    "            if sample_ids:\n",
    "                check = db.get(ids=sample_ids, include=['documents'])\n",
    "                logger.info(f\"ChromaDB Verification: Retrieved {len(check.get('documents', []))} docs.\")\n",
    "                # print(\"Sample documents from ChromaDB:\", check.get('documents'))\n",
    "            else:\n",
    "                 logger.warning(\"No sample IDs to verify ChromaDB content.\")\n",
    "        else:\n",
    "            logger.warning(\"ChromaDB collection appears empty after potential add operation.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed to setup ChromaDB: {e}\")\n",
    "        db = None # Ensure db is None if setup fails\n",
    "        print(f\"ERROR: Failed to setup ChromaDB: {e}\")\n",
    "else:\n",
    "    logger.error(\"Cannot setup ChromaDB: GOOGLE_API_KEY is missing.\")\n",
    "    print(\"ERROR: Cannot setup ChromaDB: GOOGLE_API_KEY is missing.\")\n",
    "    db = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 8 (RAG Search Function):\n",
    "- This section provides the search_menu_rag function, the core interface for querying the menu's vector database. It takes a natural language query, sets the embedding function's task type appropriately for querying (retrieval_query), executes the search against the ChromaDB collection using db.query, processes the results to return relevant document strings, and includes error handling, enabling tools to fetch contextually relevant menu information based on user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:47:00.065807Z",
     "iopub.status.busy": "2025-04-12T23:47:00.065640Z",
     "iopub.status.idle": "2025-04-12T23:47:00.302592Z",
     "shell.execute_reply": "2025-04-12T23:47:00.301896Z",
     "shell.execute_reply.started": "2025-04-12T23:47:00.065786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RAG Search results for 'Something hot with coffee and milk':\n",
      "- Latte: Espresso with steamed milk and a light layer of foam. Priced at $4.50. Available modifiers: Whole Milk, 2%, Skim, Oat Milk, Almond, Vanilla, Caramel, Hazelnut.\n",
      "- Cappuccino: Equal parts espresso, steamed milk, and foam. Priced at $4.25. Available modifiers: Whole Milk, 2%, Oat Milk, Almond, Cinnamon, Chocolate.\n",
      "- Chai Latte: Spiced tea concentrate with steamed milk. Priced at $4.50. Available modifiers: Whole Milk, 2%, Oat Milk, Almond, Extra Spicy.\n"
     ]
    }
   ],
   "source": [
    "### 8. RAG Search Function\n",
    "#\n",
    "# This function searches the ChromaDB collection. It will be called by tools that need menu context.\n",
    "\n",
    "def search_menu_rag(query: str, n_results: int = 3) -> Optional[List[str]]:\n",
    "    \"\"\"Search the menu vector database for relevant items.\"\"\"\n",
    "    if db is None:\n",
    "         logger.error(\"ChromaDB collection 'db' is not available for searching.\")\n",
    "         return None\n",
    "\n",
    "    # Set embedding function task type for querying\n",
    "    if hasattr(db, '_embedding_function') and hasattr(db._embedding_function, 'task_type'):\n",
    "         db._embedding_function.task_type = \"retrieval_query\"\n",
    "         logger.debug(f\"Set embedding task type to 'retrieval_query' for search.\")\n",
    "    else:\n",
    "         logger.warning(\"Could not set embedding task type for query.\")\n",
    "\n",
    "\n",
    "    logger.info(f\"Performing RAG search for query: '{query}' with n_results={n_results}\")\n",
    "    try:\n",
    "        results = db.query(\n",
    "            query_texts=[query],\n",
    "            n_results=min(n_results, db.count()), # Ensure n_results doesn't exceed collection size\n",
    "            include=['documents']\n",
    "        )\n",
    "        if results and isinstance(results.get('documents'), list) and results['documents']:\n",
    "            retrieved_docs = results['documents'][0]\n",
    "            logger.info(f\"RAG Search found {len(retrieved_docs)} documents.\")\n",
    "            return retrieved_docs\n",
    "        else:\n",
    "            logger.info(f\"No relevant documents found via RAG for query: '{query}'\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"ChromaDB RAG search error for query '{query}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Test (optional)\n",
    "if db:\n",
    "    test_query_rag = \"Something hot with coffee and milk\"\n",
    "    search_results_rag = search_menu_rag(test_query_rag)\n",
    "    if search_results_rag is not None:\n",
    "        print(f\"Test RAG Search results for '{test_query_rag}':\")\n",
    "        for doc in search_results_rag: print(f\"- {doc}\")\n",
    "    else: print(\"Test RAG Search failed.\")\n",
    "else: print(\"Skipping RAG search test as DB is unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 9 (LangGraph State Definition):\n",
    "- This section defines the central data structure, CafeGeniusState, using TypedDict to represent the agent's memory across turns. It includes fields for messages (conversation history, managed via add_messages), current_order (a list storing details of items the user wants), and finished (a boolean flag to signal conversation end), providing the necessary shared context for all nodes in the LangGraph graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:47:00.303391Z",
     "iopub.status.busy": "2025-04-12T23:47:00.303220Z",
     "iopub.status.idle": "2025-04-12T23:47:00.307333Z",
     "shell.execute_reply": "2025-04-12T23:47:00.306709Z",
     "shell.execute_reply.started": "2025-04-12T23:47:00.303378Z"
    }
   },
   "outputs": [],
   "source": [
    "### 9. LangGraph State Definition\n",
    "#\n",
    "# Defines the shared state object passed between nodes in the graph. It includes conversation history (`messages`), the current order details (`current_order`), and a flag (`finished`) to signal conversation end.\n",
    "\n",
    "class CafeGeniusState(TypedDict):\n",
    "    \"\"\"Represents the shared state of the CafeGenius agent.\"\"\"\n",
    "    # Conversation history. `add_messages` ensures new messages append.\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    # Current order details. List of dictionaries, e.g., {\"name\": \"Latte\", \"price\": 4.50, \"quantity\": 1, \"modifiers\": [\"Oat Milk\"]}\n",
    "    current_order: List[Dict[str, Any]]\n",
    "    # Flag to indicate if the conversation/order process is complete.\n",
    "    finished: bool\n",
    "\n",
    "logger.info(\"CafeGeniusState TypedDict defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 10 (Tool Definitions (@tool)):\n",
    "- This section defines the agent's capabilities using LangChain's @tool decorator, creating structured function schemas for the LLM. It strategically separates tools into stateless ones (get_menu, get_item_details, get_recommendations), which primarily fetch information (often using RAG), and stateful ones (add_to_order, remove_from_order, etc.), which, in this definition stage, mainly handle input validation or acknowledge intent, deferring actual state modification logic to a dedicated graph node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:47:00.309303Z",
     "iopub.status.busy": "2025-04-12T23:47:00.309130Z",
     "iopub.status.idle": "2025-04-12T23:47:00.346591Z",
     "shell.execute_reply": "2025-04-12T23:47:00.345787Z",
     "shell.execute_reply.started": "2025-04-12T23:47:00.309290Z"
    }
   },
   "outputs": [],
   "source": [
    "### 10. Tool Definitions (`@tool`)\n",
    "#\n",
    "# Define functions that the LLM can call. We use LangChain's `@tool` decorator.\n",
    "#\n",
    "# *   **Stateless Tools:** `get_menu`, `get_item_details`, `get_recommendations`. These fetch information without modifying the core `current_order` state. They will be handled by LangGraph's `ToolNode`.\n",
    "# *   **Stateful Tools:** `add_to_order`, `remove_from_order`, `clear_order`, `confirm_order`, `place_order`. These *intend* to modify the order state. While decorated with `@tool` for schema definition, their actual state modification logic resides in the custom `order_management_node`.\n",
    "\n",
    "# --- Stateless Tools ---\n",
    "\n",
    "@tool\n",
    "def get_menu() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieves the full cafe menu, organized by category, including names, prices, and modifiers for available items.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing tool: get_menu\")\n",
    "    categories: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for item in menu_data:\n",
    "        if item.get(\"available\", False):\n",
    "            category = item.get(\"category\", \"Uncategorized\")\n",
    "            if category not in categories: categories[category] = []\n",
    "            categories[category].append({\n",
    "                \"name\": item.get(\"name\", \"N/A\"),\n",
    "                \"price\": item.get(\"price\", 0.0),\n",
    "                \"modifiers\": item.get(\"modifiers\", [])\n",
    "            })\n",
    "    if not categories:\n",
    "        logger.warning(\"'get_menu' found no available items.\")\n",
    "        return {\"message\": \"Sorry, the menu seems empty right now.\"}\n",
    "    return categories\n",
    "\n",
    "@tool\n",
    "def get_item_details(item_name: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Gets detailed information (description, price, modifiers, availability) about a specific menu item by its name.\n",
    "    Uses RAG search as a fallback if an exact match isn't found.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Executing tool: get_item_details for '{item_name}'\")\n",
    "    item_name_lower = item_name.lower()\n",
    "    # Exact match first\n",
    "    for item in menu_data:\n",
    "        if item.get(\"name\", \"\").lower() == item_name_lower:\n",
    "            if item.get(\"available\", False):\n",
    "                logger.info(f\"Found exact match for available item: {item_name}\")\n",
    "                return item # Return full item details\n",
    "            else:\n",
    "                logger.warning(f\"Found exact match for '{item_name}', but it's unavailable.\")\n",
    "                return {\"name\": item_name, \"available\": False, \"message\": f\"Sorry, {item_name} is unavailable.\"}\n",
    "\n",
    "    # Fallback to RAG\n",
    "    logger.info(f\"Exact match not found for '{item_name}'. Trying RAG...\")\n",
    "    rag_query = f\"Details for menu item: {item_name}\"\n",
    "    similar_item_docs = search_menu_rag(rag_query, n_results=1)\n",
    "\n",
    "    if similar_item_docs:\n",
    "        try:\n",
    "            retrieved_name = similar_item_docs[0].split(\":\")[0].strip()\n",
    "            logger.info(f\"RAG suggested similar item: {retrieved_name}\")\n",
    "            for item in menu_data:\n",
    "                if item.get(\"name\", \"\").lower() == retrieved_name.lower() and item.get(\"available\", True):\n",
    "                     logger.info(f\"Returning details for RAG-suggested item: {retrieved_name}\")\n",
    "                     item_with_note = item.copy()\n",
    "                     item_with_note[\"note\"] = f\"Showing details for '{retrieved_name}', similar to '{item_name}'.\"\n",
    "                     return item_with_note\n",
    "        except Exception as e: logger.error(f\"Error processing RAG result: {e}\")\n",
    "\n",
    "    logger.error(f\"Item '{item_name}' could not be found.\")\n",
    "    return {\"found\": False, \"message\": f\"Sorry, I couldn't find '{item_name}' on the menu.\"}\n",
    "\n",
    "@tool\n",
    "def get_recommendations(preferences: List[str], dietary_restrictions: List[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Suggests menu items based on customer preferences (e.g., 'sweet', 'strong coffee') and optional dietary restrictions (e.g., 'vegan'), using RAG search.\n",
    "    \"\"\"\n",
    "    if dietary_restrictions is None: dietary_restrictions = []\n",
    "    logger.info(f\"Executing tool: get_recommendations. Prefs: {preferences}, Restr: {dietary_restrictions}\")\n",
    "\n",
    "    query = f\"Recommend cafe items for someone liking {', '.join(preferences)}\"\n",
    "    if dietary_restrictions: query += f\" needing options for {', '.join(dietary_restrictions)}.\"\n",
    "    query += \". Consider descriptions and ingredients.\"\n",
    "\n",
    "    relevant_descriptions = search_menu_rag(query, n_results=5)\n",
    "\n",
    "    if relevant_descriptions is None: return {\"error\": \"Recommendation search failed.\"}\n",
    "    if not relevant_descriptions: return {\"message\": \"Couldn't find specific recommendations.\"}\n",
    "\n",
    "    recommended_items_details: List[Dict[str, Any]] = []\n",
    "    seen_names = set()\n",
    "    for desc in relevant_descriptions:\n",
    "        try:\n",
    "            item_name = desc.split(\":\")[0].strip()\n",
    "            if item_name and item_name not in seen_names:\n",
    "                 item_found = next((item for item in menu_data if item.get(\"name\",\"\").lower() == item_name.lower() and item.get(\"available\", True)), None)\n",
    "                 if item_found:\n",
    "                     # Basic filtering could be added here based on restrictions\n",
    "                     recommended_items_details.append(item_found)\n",
    "                     seen_names.add(item_name)\n",
    "                 if len(recommended_items_details) >= 3: break\n",
    "        except Exception as e: logger.warning(f\"Could not parse recommendation description: {e}\")\n",
    "\n",
    "    if not recommended_items_details: return {\"message\": \"No specific recommendations found.\"}\n",
    "    else:\n",
    "         logger.info(f\"Returning {len(recommended_items_details)} recommendations.\")\n",
    "         return {\"recommendations\": recommended_items_details}\n",
    "\n",
    "# --- Stateful Tools (Schema Definition - Logic in order_management_node) ---\n",
    "\n",
    "@tool\n",
    "def add_to_order(items: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates item names and modifiers against the menu for items requested to be added to the order.\n",
    "    Does NOT modify the order state directly. Returns validation results.\n",
    "    Args: items: List of dicts, each with 'name', optional 'quantity' (default 1), optional 'modifiers' list.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Executing tool SCHEMA validation: add_to_order for {len(items)} item(s).\")\n",
    "    validation_results = []\n",
    "    overall_status = \"Validation completed.\"\n",
    "\n",
    "    for req_item in items:\n",
    "        item_name = req_item.get(\"name\")\n",
    "        quantity = req_item.get(\"quantity\", 1)\n",
    "        req_mods = req_item.get(\"modifiers\", [])\n",
    "        result = {\"requested_name\": item_name, \"requested_quantity\": quantity, \"requested_modifiers\": req_mods, \"status\": \"pending\"}\n",
    "\n",
    "        if not item_name or quantity < 1:\n",
    "            result[\"status\"] = \"error\"\n",
    "            result[\"reason\"] = \"Missing name or invalid quantity.\"\n",
    "            validation_results.append(result)\n",
    "            continue\n",
    "\n",
    "        menu_item = next((m for m in menu_data if m.get(\"name\",\"\").lower() == item_name.lower()), None)\n",
    "\n",
    "        if menu_item and menu_item.get(\"available\"):\n",
    "            valid_mods = []\n",
    "            invalid_mods = []\n",
    "            available_mods = [m.lower() for m in menu_item.get(\"modifiers\", [])]\n",
    "            for mod in req_mods:\n",
    "                if mod.lower() in available_mods:\n",
    "                    # Find canonical capitalization\n",
    "                    canonical_mod = next((m for m in menu_item[\"modifiers\"] if m.lower() == mod.lower()), mod)\n",
    "                    valid_mods.append(canonical_mod)\n",
    "                else:\n",
    "                    invalid_mods.append(mod)\n",
    "\n",
    "            result[\"status\"] = \"valid\"\n",
    "            result[\"validated_item\"] = {\n",
    "                \"name\": menu_item[\"name\"],\n",
    "                \"price\": menu_item[\"price\"],\n",
    "                \"quantity\": quantity,\n",
    "                \"modifiers\": valid_mods\n",
    "            }\n",
    "            if invalid_mods:\n",
    "                result[\"warning\"] = f\"Invalid modifiers ignored: {invalid_mods}\"\n",
    "                logger.warning(f\"Invalid modifiers for '{item_name}': {invalid_mods}\")\n",
    "        elif menu_item:\n",
    "            result[\"status\"] = \"error\"\n",
    "            result[\"reason\"] = f\"Item '{item_name}' is unavailable.\"\n",
    "            logger.warning(f\"Item '{item_name}' requested but unavailable.\")\n",
    "        else:\n",
    "            result[\"status\"] = \"error\"\n",
    "            result[\"reason\"] = f\"Item '{item_name}' not found on menu.\"\n",
    "            logger.warning(f\"Item '{item_name}' requested but not found.\")\n",
    "\n",
    "        validation_results.append(result)\n",
    "\n",
    "    # Consolidate overall status message if errors occurred\n",
    "    if any(res[\"status\"] == \"error\" for res in validation_results):\n",
    "        overall_status = \"Validation completed with errors.\"\n",
    "    elif any(res.get(\"warning\") for res in validation_results):\n",
    "        overall_status = \"Validation completed with warnings (invalid modifiers).\"\n",
    "\n",
    "    logger.info(f\"Tool add_to_order validation result: {overall_status}\")\n",
    "    return {\"overall_status\": overall_status, \"details\": validation_results}\n",
    "\n",
    "\n",
    "@tool\n",
    "def remove_from_order(item_name: str, quantity: int = 1) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Specifies an item and quantity to be removed from the order.\n",
    "    Does NOT modify the order state directly. Logic is in order_management_node.\n",
    "    Args: item_name: Name of the item to remove. quantity: How many to remove (default 1).\n",
    "    \"\"\"\n",
    "    logger.info(f\"Executing tool SCHEMA: remove_from_order request for {quantity}x '{item_name}'\")\n",
    "    # No real logic here, just acknowledges the request for the LLM.\n",
    "    # The actual removal happens in the stateful node.\n",
    "    return {\"status\": \"Removal request acknowledged.\", \"item_name\": item_name, \"quantity\": quantity}\n",
    "\n",
    "@tool\n",
    "def clear_order() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Requests to clear all items from the current order.\n",
    "    Does NOT modify the order state directly. Logic is in order_management_node.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing tool SCHEMA: clear_order request\")\n",
    "    return {\"status\": \"Clear order request acknowledged.\"}\n",
    "\n",
    "@tool\n",
    "def confirm_order() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Requests confirmation of the current order from the user.\n",
    "    The order_management_node will display the order and collect user input.\n",
    "    Returns the user's confirmation response ('yes', 'no', or suggested changes).\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing tool SCHEMA: confirm_order request\")\n",
    "    # The actual display/input happens in the order_management_node\n",
    "    return {\"status\": \"Confirmation process initiated.\"}\n",
    "\n",
    "@tool\n",
    "def place_order() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Requests to finalize and place the current order.\n",
    "    The order_management_node will simulate sending the order and provide an ETA.\n",
    "    \"\"\"\n",
    "    logger.info(\"Executing tool SCHEMA: place_order request\")\n",
    "    # The actual placing/ETA simulation happens in the order_management_node\n",
    "    return {\"status\": \"Order placement process initiated.\"}\n",
    "\n",
    "\n",
    "all_tools = [\n",
    "    get_menu, get_item_details, get_recommendations, # Stateless\n",
    "    add_to_order, remove_from_order, clear_order, confirm_order, place_order # Stateful (Schema only)\n",
    "]\n",
    "stateless_tool_list = [get_menu, get_item_details, get_recommendations]\n",
    "stateful_tool_names = {t.name for t in all_tools} - {t.name for t in stateless_tool_list}\n",
    "\n",
    "logger.info(f\"Defined {len(all_tools)} tools. Stateless: {len(stateless_tool_list)}, Stateful (Schema): {len(stateful_tool_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 11 (LLM and System Prompt Configuration):\n",
    "- This section configures the core intelligence of the agent by initializing the ChatGoogleGenerativeAI model (Gemini) and binding all previously defined tools using .bind_tools(), enabling the LLM to invoke them via function calling. It also defines a detailed system prompt (BARISTABOT_SYSINT) that instructs the LLM on its persona, objectives, rules for tool usage (especially stateful ones like confirm/place order), desired tone, and how to handle limitations, guiding its behavior within the agent framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:47:00.347652Z",
     "iopub.status.busy": "2025-04-12T23:47:00.347401Z",
     "iopub.status.idle": "2025-04-12T23:47:00.395357Z",
     "shell.execute_reply": "2025-04-12T23:47:00.394591Z",
     "shell.execute_reply.started": "2025-04-12T23:47:00.347634Z"
    }
   },
   "outputs": [],
   "source": [
    "### 11. LLM and System Prompt Configuration\n",
    "#\n",
    "# Initialize the LangChain ChatGoogleGenerativeAI model and bind the tools to it. Define the system prompt.\n",
    "\n",
    "# Use a Gemini model suitable for function calling\n",
    "# model_name = \"gemini-1.5-flash-latest\" # Faster, good quota\n",
    "model_name=\"gemini-2.0-flash\" # Or explore gemini-1.5-pro-latest for complex reasoning\n",
    "llm = ChatGoogleGenerativeAI(model=model_name,\n",
    "                             convert_system_message_to_human=False # Helps some models adhere better\n",
    "                            )\n",
    "\n",
    "# Bind ALL tools so the LLM knows their schemas\n",
    "llm_with_tools = llm.bind_tools(all_tools)\n",
    "logger.info(f\"Initialized LLM '{model_name}' and bound {len(all_tools)} tools.\")\n",
    "\n",
    "# System prompt - Guiding the LLM's behavior\n",
    "# Updated to include new tools like remove, clear, confirm, place\n",
    "BARISTABOT_SYSINT = \"\"\"You are CafeGenius (LangGraph Edition), a cheerful and efficient AI assistant for our cafe. Your goal is to help customers explore the menu, get recommendations, build their order, and finalize it.\n",
    "\n",
    "**Key Instructions:**\n",
    "*   **Use Tools:** Rely on your available tools (`get_menu`, `get_item_details`, `get_recommendations`, `add_to_order`, `remove_from_order`, `clear_order`, `confirm_order`, `place_order`) for all actions related to the menu and order. Do NOT guess menu details, prices, or availability.\n",
    "*   **Order Management:**\n",
    "    *   Use `add_to_order` to add items. The tool will validate them; report the outcome clearly (what was added, what failed and why).\n",
    "    *   Use `remove_from_order` if the user asks to remove something.\n",
    "    *   Use `clear_order` if the user wants to start over.\n",
    "    *   Before placing, use `confirm_order`. This tool will handle showing the order to the user and getting their 'yes', 'no', or requested changes. Relay the user's response accurately.\n",
    "    *   Only use `place_order` AFTER the user explicitly confirms via the `confirm_order` tool ('yes'). The `place_order` tool will simulate sending the order and provide an ETA.\n",
    "*   **Clarity & RAG:** Use `get_item_details` and `get_recommendations` which leverage RAG for accuracy. Format responses clearly (Markdown lists). Explain recommendations based on user input.\n",
    "*   **Tone:** Be friendly, polite, and helpful. Keep responses concise.\n",
    "*   **Limitations:** If a tool reports an error or can't find information, politely inform the user and suggest alternatives (e.g., check the menu, rephrase). Do not make up information. If the user asks to remove/clear/confirm/place order but you don't call the respective tool, explain you need to use your tools for that action.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 12 (Node Definitions):\n",
    "- This section implements the core processing units of the LangGraph state machine as Python functions (or prebuilt classes). It defines the chatbot_node (calls LLM), human_node (handles user I/O), tools_node (an instance of ToolNode for automatically executing stateless tools), and the critical order_management_node (a custom function executing the actual state-modification logic for tools like adding/removing/clearing/confirming/placing orders based on LLM tool calls and returning results as ToolMessage objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:47:00.396265Z",
     "iopub.status.busy": "2025-04-12T23:47:00.396069Z",
     "iopub.status.idle": "2025-04-12T23:47:00.416284Z",
     "shell.execute_reply": "2025-04-12T23:47:00.415581Z",
     "shell.execute_reply.started": "2025-04-12T23:47:00.396250Z"
    }
   },
   "outputs": [],
   "source": [
    "### 12. Node Definitions\n",
    "#\n",
    "# Define the Python functions that will act as nodes in our LangGraph state machine.\n",
    "#\n",
    "# *   `chatbot_node`: Interacts with the LLM.\n",
    "# *   `human_node`: Handles user input/output.\n",
    "# *   `tools_node`: Executes stateless tools automatically (using `ToolNode`).\n",
    "# *   `order_management_node`: Executes the logic for stateful order tools (`add`, `remove`, `clear`, `confirm`, `place`).\n",
    "\n",
    "# --- Chatbot Node ---\n",
    "def chatbot_node(state: CafeGeniusState) -> Dict[str, Any]:\n",
    "    \"\"\"Invokes the LLM with the current state (messages) and tools.\"\"\"\n",
    "    logger.info(\"Entering chatbot_node\")\n",
    "    # Prepend system message to the history if it's not already there implicitly\n",
    "    # Note: ChatGoogleGenerativeAI might handle system message differently depending on version/settings.\n",
    "    # If BARISTABOT_SYSINT is a string: messages_for_llm = [SystemMessage(content=BARISTABOT_SYSINT)] + state['messages']\n",
    "    # If BARISTABOT_SYSINT is already a tuple ('system', 'content'): LangChain expects BaseMessage objects.\n",
    "    # Let's rely on ChatGoogleGenerativeAI's `system_instruction` parameter if available, or prepend manually if needed.\n",
    "    # For simplicity here, assume ChatGoogleGenerativeAI handles it or convert_system_message_to_human=True helps.\n",
    "\n",
    "    #if not state['messages']:\n",
    "    #    # Handle the very first turn - send a welcome message\n",
    "    #    logger.info(\"Chatbot_node: No messages yet, returning welcome message.\")\n",
    "    #    welcome = AIMessage(content=\"Welcome to CafeGenius (LangGraph Edition)! How can I help you today? (Type 'menu', 'recommend', or ask questions. Type 'exit' to quit.)\")\n",
    "    #    return {\"messages\": [welcome]}\n",
    "    #else:\n",
    "\n",
    "    if not state['messages']:\n",
    "        # Handle the very first turn *after* the banner has been printed.\n",
    "        # Return a simple prompt to start the actual interaction.\n",
    "        logger.info(\"Chatbot_node: No messages yet (after banner), returning initial prompt.\")\n",
    "        # The banner already asked \"How can I brighten your day?\"\n",
    "        # So, we don't strictly need *another* message here before the first user input.\n",
    "        # However, the `human_node` expects *some* AI message to display before prompting.\n",
    "        # Let's return a minimal confirmation.\n",
    "        initial_prompt = AIMessage(content=\"I'm ready for your request!\") # Simple message\n",
    "        return {\"messages\": [initial_prompt]}\n",
    "    else:\n",
    "    \n",
    "         logger.info(f\"Chatbot_node: Invoking LLM with {len(state['messages'])} history messages.\")\n",
    "         try:\n",
    "            # Invoke LLM with bound tools\n",
    "            response = llm_with_tools.invoke(state['messages'])\n",
    "            logger.info(\"Chatbot_node: Received LLM response.\")\n",
    "            # The response should be an AIMessage, potentially with tool_calls\n",
    "            return {\"messages\": [response]}\n",
    "         except Exception as e:\n",
    "             logger.exception(\"Chatbot_node: Error invoking LLM.\")\n",
    "             error_message = AIMessage(content=f\"Sorry, I encountered an error processing your request: {e}\")\n",
    "             return {\"messages\": [error_message]}\n",
    "\n",
    "\n",
    "# --- Human Interaction Node ---\n",
    "def human_node(state: CafeGeniusState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Displays the latest AI message and prompts the user for input,\n",
    "    ensuring non-empty input before proceeding.\n",
    "    \"\"\"\n",
    "    logger.info(\"Entering human_node\")\n",
    "    last_message = state['messages'][-1]\n",
    "\n",
    "    # Display AI message(s) since last human input (Existing logic - unchanged)\n",
    "    ai_messages_content = []\n",
    "    for msg in reversed(state['messages']):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            break\n",
    "        if isinstance(msg, AIMessage):\n",
    "            if msg.content:\n",
    "                ai_messages_content.append(msg.content)\n",
    "\n",
    "    if ai_messages_content:\n",
    "        print(\"\\nCafeGenius:\")\n",
    "        for content in reversed(ai_messages_content):\n",
    "            display(Markdown(content))\n",
    "    else:\n",
    "        logger.warning(\"Human_node: No new AI messages to display.\")\n",
    "\n",
    "    # --- Modified Input Loop ---\n",
    "    user_input = \"\"\n",
    "    while not user_input: # Loop until non-empty, non-whitespace input is received\n",
    "        user_input = input(\"\\nYou: \").strip() # Use strip()!\n",
    "        if not user_input:\n",
    "            # Provide feedback if input is empty\n",
    "            print(\"CafeGenius: Please provide some input, or type 'exit' to quit.\")\n",
    "        # Allow exit commands even if they are the only input after stripping\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]:\n",
    "            break # Exit the loop if user wants to quit\n",
    "\n",
    "    print(\"-\" * 28) # Separator\n",
    "\n",
    "    # --- Exit Handling (Existing logic - largely unchanged) ---\n",
    "    finished = False\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]:\n",
    "        print(\"CafeGenius: Thanks for visiting! Have a great day!\")\n",
    "        finished = True\n",
    "        logger.info(\"Human_node: User initiated exit.\")\n",
    "        # Ensure exit commands still result in a message if needed by logic downstream,\n",
    "        # although the 'finished' flag is the primary control here.\n",
    "        # If user typed 'exit', user_input is now 'exit', which is not empty.\n",
    "\n",
    "    # --- Return Validated Input ---\n",
    "    # Now, user_input is guaranteed to be non-empty unless it's a valid exit command.\n",
    "    return {\"messages\": [HumanMessage(content=user_input)], \"finished\": finished}\n",
    "    \n",
    "\n",
    "# --- Stateless Tools Node ---\n",
    "# Uses LangGraph's prebuilt ToolNode for automatic execution [2]\n",
    "tools_node = ToolNode(stateless_tool_list)\n",
    "logger.info(f\"Created ToolNode for {len(stateless_tool_list)} stateless tools.\")\n",
    "\n",
    "\n",
    "# --- Stateful Order Management Node ---\n",
    "def order_management_node(state: CafeGeniusState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Executes the logic for stateful order tools based on LLM tool calls.\n",
    "    Modifies state['current_order'] and state['finished'].\n",
    "    Returns ToolMessage results for the LLM.\n",
    "    \"\"\"\n",
    "    logger.info(\"Entering order_management_node\")\n",
    "    last_message = state['messages'][-1]\n",
    "    if not isinstance(last_message, AIMessage) or not last_message.tool_calls:\n",
    "        logger.warning(\"Order_management_node: Expected AIMessage with tool_calls, none found.\")\n",
    "        return {} # Should not happen if routing is correct\n",
    "\n",
    "    tool_messages = []\n",
    "    # Get current order, default to empty list if not present\n",
    "    current_order = state.get('current_order', [])\n",
    "    # Ensure finished state persists or defaults to False\n",
    "    finished = state.get('finished', False)\n",
    "\n",
    "    logger.info(f\"Order_management_node: Processing {len(last_message.tool_calls)} tool calls.\")\n",
    "\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool_name = tool_call['name']\n",
    "        tool_args = tool_call['args']\n",
    "        tool_call_id = tool_call['id']\n",
    "        response_content = {\"error\": f\"Execution failed for tool '{tool_name}'.\"} # Default error\n",
    "\n",
    "        logger.info(f\"Order_management_node: Executing logic for tool '{tool_name}' with args: {tool_args}\")\n",
    "\n",
    "        try:\n",
    "            if tool_name == \"add_to_order\":\n",
    "                # 1. Invoke the @tool function to get validation results\n",
    "                validation_response = add_to_order.invoke(tool_args)\n",
    "                items_added_count = 0\n",
    "                # 2. Process validation results and update state['current_order']\n",
    "                if isinstance(validation_response, dict) and \"details\" in validation_response:\n",
    "                    for detail in validation_response[\"details\"]:\n",
    "                        if detail.get(\"status\") == \"valid\" and \"validated_item\" in detail:\n",
    "                             # Add validated item to the actual order state\n",
    "                             item_to_add = detail[\"validated_item\"]\n",
    "\n",
    "                             # Check if item already exists to update quantity/merge\n",
    "                             found = False\n",
    "                             for existing_item in current_order:\n",
    "                                # Simple merge: same name and same modifiers (order doesn't matter)\n",
    "                                if existing_item['name'] == item_to_add['name'] and \\\n",
    "                                   set(existing_item.get('modifiers', [])) == set(item_to_add.get('modifiers', [])):\n",
    "                                    existing_item['quantity'] += item_to_add['quantity']\n",
    "                                    found = True\n",
    "                                    break\n",
    "                             if not found:\n",
    "                                current_order.append(item_to_add)\n",
    "\n",
    "                             items_added_count += 1\n",
    "                             logger.info(f\"Order Node: Added/Updated '{item_to_add['name']}' (Qty: {item_to_add['quantity']}) to state.\")\n",
    "\n",
    "                    # 3. Return the validation result itself as the ToolMessage content\n",
    "                    response_content = validation_response\n",
    "                    response_content[\"summary\"] = f\"{items_added_count} item(s) successfully processed for adding.\"\n",
    "\n",
    "                else:\n",
    "                    response_content = {\"error\": \"Validation function returned unexpected format.\"}\n",
    "\n",
    "            elif tool_name == \"remove_from_order\":\n",
    "                item_name_to_remove = tool_args.get(\"item_name\")\n",
    "                qty_to_remove = tool_args.get(\"quantity\", 1)\n",
    "                removed_count = 0\n",
    "                new_order = []\n",
    "                processed_indices = set() # Track indices already processed\n",
    "\n",
    "                # Iterate through order to find matching items\n",
    "                for i, item in enumerate(current_order):\n",
    "                     if i in processed_indices: continue # Skip if already processed\n",
    "\n",
    "                     # Match by name (case-insensitive) - requires exact name match for removal\n",
    "                     if item['name'].lower() == item_name_to_remove.lower() and removed_count < qty_to_remove:\n",
    "                         if item['quantity'] > qty_to_remove - removed_count:\n",
    "                             # Reduce quantity of this item entry\n",
    "                             item['quantity'] -= (qty_to_remove - removed_count)\n",
    "                             new_order.append(item)\n",
    "                             removed_count = qty_to_remove # Target met\n",
    "                             processed_indices.add(i)\n",
    "                         else:\n",
    "                             # Remove this entire item entry or reduce quantity to remove\n",
    "                             removed_count += item['quantity']\n",
    "                             # Do not append item to new_order\n",
    "                             processed_indices.add(i)\n",
    "                     else:\n",
    "                        # Keep item if it doesn't match or removal target met\n",
    "                        if i not in processed_indices:\n",
    "                             new_order.append(item)\n",
    "                             processed_indices.add(i) # Mark as processed even if kept\n",
    "\n",
    "                current_order = new_order # Update the order list in state\n",
    "                logger.info(f\"Order Node: Attempted remove '{item_name_to_remove}'. Removed: {removed_count}. New count: {len(current_order)}\")\n",
    "                response_content = {\"status\": f\"Removed {removed_count} of '{item_name_to_remove}'.\", \"remaining_order_count\": len(current_order)}\n",
    "\n",
    "            elif tool_name == \"clear_order\":\n",
    "                 current_order = []\n",
    "                 logger.info(\"Order Node: Order cleared.\")\n",
    "                 response_content = {\"status\": \"Order cleared successfully.\"}\n",
    "\n",
    "            elif tool_name == \"confirm_order\":\n",
    "                 # Display current order and ask for confirmation\n",
    "                 order_text = \"**Confirming Your Order:**\\n\\n\"\n",
    "                 total = sum(item.get(\"price\", 0.0) * item.get(\"quantity\", 1) for item in current_order)\n",
    "                 if not current_order:\n",
    "                     order_text += \"Your order is currently empty.\"\n",
    "                 else:\n",
    "                     for item in current_order:\n",
    "                         mods = f\" (Modifiers: {', '.join(item.get('modifiers',[]))})\" if item.get('modifiers') else \"\"\n",
    "                         order_text += f\"- {item['quantity']}x {item['name']}{mods}: ${item['price']*item['quantity']:.2f}\\n\"\n",
    "                     order_text += f\"\\n**Total: ${total:.2f}**\"\n",
    "\n",
    "                 print(\"\\nCafeGenius:\")\n",
    "                 display(Markdown(order_text)) # Show order to user\n",
    "                 confirmation = input(\"Is this correct? (yes / no / or type changes): \").lower().strip()\n",
    "                 logger.info(f\"Order Node: User confirmation response: '{confirmation}'\")\n",
    "                 # Send the user's raw response back to the LLM\n",
    "                 response_content = {\"user_confirmation_response\": confirmation}\n",
    "\n",
    "            elif tool_name == \"place_order\":\n",
    "                 # Simulate placing the order\n",
    "                 final_order_text = \"**Placing Final Order:**\\n\"\n",
    "                 total = sum(item.get(\"price\", 0.0) * item.get(\"quantity\", 1) for item in current_order)\n",
    "                 if not current_order:\n",
    "                     final_order_text += \"Order is empty. Nothing placed.\"\n",
    "                     response_content = {\"status\": \"Order was empty, nothing placed.\", \"estimated_wait_minutes\": 0}\n",
    "                 else:\n",
    "                     for item in current_order:\n",
    "                         mods = f\" ({', '.join(item.get('modifiers',[]))})\" if item.get('modifiers') else \"\"\n",
    "                         final_order_text += f\"- {item['quantity']}x {item['name']}{mods}\\n\"\n",
    "                     final_order_text += f\"Total: ${total:.2f}\\n\\nOrder sent to the kitchen!\"\n",
    "                     eta = randint(2, 7) # Simulate ETA\n",
    "                     final_order_text += f\"\\nEstimated wait time: {eta} minutes.\"\n",
    "                     response_content = {\"status\": \"Order placed successfully!\", \"estimated_wait_minutes\": eta}\n",
    "                     finished = True # Set flag to end conversation after placing\n",
    "\n",
    "                 print(\"\\nCafeGenius:\")\n",
    "                 display(Markdown(final_order_text))\n",
    "                 logger.info(f\"Order Node: Placed order. ETA: {response_content.get('estimated_wait_minutes')}. Setting finished=True.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error executing order management logic for {tool_name}: {e}\")\n",
    "            response_content['error'] = f\"Internal error processing {tool_name}: {str(e)}\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # Append result as ToolMessage\n",
    "        # Ensure content is JSON serializable (dicts usually are)\n",
    "        try:\n",
    "            # Convert complex objects (like Pydantic models if used) to dicts if necessary\n",
    "            serializable_content = json.dumps(response_content)\n",
    "        except TypeError:\n",
    "             logger.error(f\"Failed to serialize response content for {tool_name}. Content: {response_content}\")\n",
    "             serializable_content = json.dumps({\"error\": f\"Failed to serialize tool result for {tool_name}.\"})\n",
    "\n",
    "        tool_messages.append(ToolMessage(content=serializable_content, tool_call_id=tool_call_id))\n",
    "\n",
    "    # Return updated state components\n",
    "    return {\"messages\": tool_messages, \"current_order\": current_order, \"finished\": finished}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 13 (Graph Definition and Routing):\n",
    "- This section constructs the agent's explicit control flow using LangGraph's StateGraph. It adds all defined nodes, sets the starting point (START), defines conditional edges using routing functions (route_from_chatbot, route_from_human) that direct the flow based on the current state (e.g., presence/type of tool calls, finished flag), adds unconditional edges for fixed transitions (e.g., tools -> chatbot), and finally compiles the entire structure into a runnable app object, optionally visualizing the resulting state machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:47:00.417171Z",
     "iopub.status.busy": "2025-04-12T23:47:00.416974Z",
     "iopub.status.idle": "2025-04-12T23:47:00.502242Z",
     "shell.execute_reply": "2025-04-12T23:47:00.501625Z",
     "shell.execute_reply.started": "2025-04-12T23:47:00.417156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 13. Graph Definition and Routing\n",
    "#\n",
    "# Construct the `StateGraph` by adding nodes and defining the edges (transitions) between them, including conditional routing logic.\n",
    "\n",
    "graph_builder = StateGraph(CafeGeniusState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
    "graph_builder.add_node(\"human_input\", human_node)\n",
    "graph_builder.add_node(\"stateless_tools\", tools_node)\n",
    "graph_builder.add_node(\"order_management\", order_management_node)\n",
    "\n",
    "logger.info(\"Nodes added to graph builder.\")\n",
    "\n",
    "# Define edges and conditional routing logic\n",
    "\n",
    "# --- Entry Point ---\n",
    "graph_builder.add_edge(START, \"chatbot\") # Start with the chatbot (which will send welcome)\n",
    "\n",
    "# --- Routing from Chatbot ---\n",
    "def route_from_chatbot(state: CafeGeniusState) -> Literal[\"stateless_tools\", \"order_management\", \"human_input\", \"__end__\"]:\n",
    "    \"\"\"Determines the next node after the chatbot generates a response.\"\"\"\n",
    "    logger.debug(\"Executing route_from_chatbot\")\n",
    "    if state.get(\"finished\"):\n",
    "        logger.debug(\"Routing from chatbot to END (finished flag is set).\")\n",
    "        return END\n",
    "    last_message = state['messages'][-1]\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        # Check if any tool call requires the stateful order_management node\n",
    "        if any(tc['name'] in stateful_tool_names for tc in last_message.tool_calls):\n",
    "            logger.debug(\"Routing from chatbot to order_management.\")\n",
    "            return \"order_management\"\n",
    "        else:\n",
    "            # Assume other tool calls are handled by the stateless tools node\n",
    "            logger.debug(\"Routing from chatbot to stateless_tools.\")\n",
    "            return \"stateless_tools\"\n",
    "    else:\n",
    "        # No tool calls, or not an AIMessage -> go to human for input\n",
    "        logger.debug(\"Routing from chatbot to human_input.\")\n",
    "        return \"human_input\"\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_from_chatbot,\n",
    "    {\n",
    "        \"stateless_tools\": \"stateless_tools\",\n",
    "        \"order_management\": \"order_management\",\n",
    "        \"human_input\": \"human_input\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Routing from Human Input ---\n",
    "def route_from_human(state: CafeGeniusState) -> Literal[\"chatbot\", \"__end__\"]:\n",
    "    \"\"\"Determines the next node after human input.\"\"\"\n",
    "    logger.debug(\"Executing route_from_human\")\n",
    "    if state.get(\"finished\"):\n",
    "        logger.debug(\"Routing from human_input to END (finished flag is set).\")\n",
    "        return END\n",
    "    else:\n",
    "        # Always go back to the chatbot to process the human input\n",
    "        logger.debug(\"Routing from human_input to chatbot.\")\n",
    "        return \"chatbot\"\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"human_input\",\n",
    "    route_from_human,\n",
    "    {\n",
    "        \"chatbot\": \"chatbot\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- Routing from Tool Nodes ---\n",
    "# Both stateless and stateful tool nodes always go back to the chatbot\n",
    "# for it to process the tool results.\n",
    "graph_builder.add_edge(\"stateless_tools\", \"chatbot\")\n",
    "graph_builder.add_edge(\"order_management\", \"chatbot\")\n",
    "\n",
    "logger.info(\"Edges and conditional edges defined.\")\n",
    "\n",
    "# --- Compile the Graph ---\n",
    "try:\n",
    "    app = graph_builder.compile()\n",
    "    logger.info(\"LangGraph compiled successfully.\")\n",
    "\n",
    "    # Optional: Visualize the graph\n",
    "    try:\n",
    "        graph_png = app.get_graph().draw_mermaid_png()\n",
    "        display(Image(graph_png))\n",
    "        logger.info(\"Graph visualization generated.\")\n",
    "    except Exception as viz_error:\n",
    "        logger.warning(f\"Could not generate graph visualization: {viz_error}\")\n",
    "\n",
    "except Exception as compile_error:\n",
    "    logger.exception(\"Failed to compile LangGraph.\")\n",
    "    print(f\"ERROR: Failed to compile graph: {compile_error}\")\n",
    "    app = None # Ensure app is None if compilation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:47:00.503777Z",
     "iopub.status.busy": "2025-04-12T23:47:00.503442Z",
     "iopub.status.idle": "2025-04-12T23:47:00.509119Z",
     "shell.execute_reply": "2025-04-12T23:47:00.508377Z",
     "shell.execute_reply.started": "2025-04-12T23:47:00.503756Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Function to Print Welcome Banner ---\n",
    "def print_welcome_banner(model_name: str):\n",
    "    \"\"\"Prints the detailed welcome banner.\"\"\"\n",
    "    banner_lines = [\n",
    "        f\"--> CafeGenius Assistant initializing with model: {model_name}\",\n",
    "        \"╔════════════════════════════════════════════════════════════════════════════════╗\",\n",
    "        \"║        ⭐ Welcome to CafeGenius Assistant v1.2 (LangGraph Edition)! ⭐         ║\", \n",
    "        \"║            (Capstone Project: Kaggle x Google GenAI Intensive 2025)            ║\",\n",
    "        \"║                (Developer: Erwin R. Pasia | erwinpasia@gmail.com)              ║\",\n",
    "        \"║                                                                                ║\",\n",
    "        \"║  Hi there! I'm your virtual barista, ready to help.                            ║\",\n",
    "        \"║  You can ask me about the menu, get recommendations, or manage your order.     ║\",\n",
    "        \"║                                                                                ║\",\n",
    "        \"║                                                                                ║\",\n",
    "        \"║  How can I brighten your day?                                                  ║\",\n",
    "        \"╚════════════════════════════════════════════════════════════════════════════════╝\"\n",
    "    ]\n",
    "    print(\"\\n\".join(banner_lines))\n",
    "    print(\"-\" * (len(banner_lines[1]))) # Separator line matching box width\n",
    "\n",
    "logger.info(\"Welcome banner function defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 14 (Running the CafeGenius Agent):\n",
    "- This final section provides the interactive runtime environment for the agent. It initializes the agent's state, displays a welcome banner, and enters a while loop that repeatedly calls app.invoke(current_state) to execute the compiled LangGraph graph. The loop updates the state based on the graph's output after each turn (which includes running the LLM, tools, and prompting the user via the human_node), continuing until the state's finished flag becomes true (e.g., after placing an order) or the user exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T01:37:47.023304Z",
     "iopub.status.busy": "2025-04-13T01:37:47.023068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> CafeGenius Assistant initializing with model: gemini-2.0-flash\n",
      "╔════════════════════════════════════════════════════════════════════════════════╗\n",
      "║        ⭐ Welcome to CafeGenius Assistant v1.2 (LangGraph Edition)! ⭐         ║\n",
      "║            (Capstone Project: Kaggle x Google GenAI Intensive 2025)            ║\n",
      "║                (Developer: Erwin R. Pasia | erwinpasia@gmail.com)              ║\n",
      "║                                                                                ║\n",
      "║  Hi there! I'm your virtual barista, ready to help.                            ║\n",
      "║  You can ask me about the menu, get recommendations, or manage your order.     ║\n",
      "║                                                                                ║\n",
      "║                                                                                ║\n",
      "║  How can I brighten your day?                                                  ║\n",
      "╚════════════════════════════════════════════════════════════════════════════════╝\n",
      "----------------------------------------------------------------------------------\n",
      "\n",
      "CafeGenius:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm ready for your request!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 14. Running the CafeGenius Agent\n",
    "#\n",
    "# This cell initiates the chat loop. It uses `app.invoke` within a loop to manage the conversation turn-by-turn, similar to the structure in the LangGraph example notebook. \n",
    "# State is manually passed between invocations.\n",
    "#\n",
    "# **Note:** Unlike the original notebook's `chat_with_cafe_genius` function which contained the entire loop logic, here the loop *orchestrates* calls to the compiled LangGraph `app`. \n",
    "# The graph itself handles the internal transitions between chatbot, tools, and human input prompts *within* a single `invoke` call until it requires the next external input.\n",
    "\n",
    "if app and GOOGLE_API_KEY:\n",
    "    logger.info(\"Starting interactive chat session...\")\n",
    "\n",
    "    # Print the detailed welcome banner ONCE before the loop\n",
    "    print_welcome_banner(model_name) # Pass the actual model name variable used\n",
    "    # ---------------------\n",
    "    \n",
    "    # Initial state for the conversation\n",
    "    current_state = CafeGeniusState(messages=[], current_order=[], finished=False)\n",
    "\n",
    "    # Configuration for invoke\n",
    "    config = {\"recursion_limit\": 150} # Increase recursion depth for complex interactions\n",
    "\n",
    "    while not current_state.get(\"finished\", False):\n",
    "        try:\n",
    "            # Invoke the graph with the current state\n",
    "            # The graph will run until it hits the 'human_input' node or the END node.\n",
    "            result_state = app.invoke(current_state, config=config)\n",
    "\n",
    "            # Update the current state with the result from the graph execution\n",
    "            # This includes new messages, potentially updated order, and finished status\n",
    "            current_state = result_state\n",
    "\n",
    "            # Check finished flag again after invocation (e.g., if place_order was called)\n",
    "            if current_state.get(\"finished\", False):\n",
    "                 logger.info(\"Chat loop ending because 'finished' flag is true in state.\")\n",
    "                 if current_state.get('current_order'):\n",
    "                      print(\"\\nFinal Order Summary:\")\n",
    "                      final_order_text = \"\"\n",
    "                      total = sum(item.get(\"price\", 0.0) * item.get(\"quantity\", 1) for item in current_state['current_order'])\n",
    "                      for item in current_state['current_order']:\n",
    "                          mods = f\" ({', '.join(item.get('modifiers',[]))})\" if item.get('modifiers') else \"\"\n",
    "                          final_order_text += f\"- {item['quantity']}x {item['name']}{mods}: ${item['price']*item['quantity']:.2f}\\n\"\n",
    "                      final_order_text += f\"\\n**Total: ${total:.2f}**\"\n",
    "                      display(Markdown(final_order_text))\n",
    "                 else:\n",
    "                      print(\"\\nNo final order.\")\n",
    "                 break # Exit the while loop\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nCafeGenius: Exiting chat session.\")\n",
    "            logger.info(\"Chat session interrupted by user (KeyboardInterrupt).\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"An error occurred during the chat loop invocation: {e}\")\n",
    "            print(f\"\\n🚨 CafeGenius: Oops! Something went wrong: {str(e)}. Please try again or type 'exit'.\")\n",
    "            # Optional: break or attempt to recover state\n",
    "            # For simplicity, we continue, but state might be inconsistent\n",
    "            # current_state['messages'].append(AIMessage(content=f\"Sorry, an error occurred: {e}\")) # Inform LLM?\n",
    "\n",
    "    logger.info(\"Interactive chat session finished.\")\n",
    "\n",
    "elif not GOOGLE_API_KEY:\n",
    "    print(\"Cannot start chat: GOOGLE_API_KEY is missing or invalid.\")\n",
    "    logger.error(\"Chat session not started due to missing API key.\")\n",
    "elif not app:\n",
    "    print(\"Cannot start chat: Graph compilation failed.\")\n",
    "    logger.error(\"Chat session not started due to graph compilation failure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T23:48:54.963770Z",
     "iopub.status.busy": "2025-04-12T23:48:54.963627Z",
     "iopub.status.idle": "2025-04-12T23:48:54.967366Z",
     "shell.execute_reply": "2025-04-12T23:48:54.966798Z",
     "shell.execute_reply.started": "2025-04-12T23:48:54.963759Z"
    }
   },
   "outputs": [],
   "source": [
    "#╔════════════════════════════════════════════════════════════════════════════════╗#\n",
    "#║  Quick Commands / Or Just Ask Naturally:                                         ║#\n",
    "#║    'menu'                      -> View our offerings                             ║#\n",
    "#║    'details Latte'             -> Get info on Latte                              ║#\n",
    "#║    'recommend warm sweet'      -> Get suggestions for warm and sweet items       ║#\n",
    "#║    'add 2 Cappuccino Oat Milk' -> Add 2 Cappuccinos with Oat Milk                ║#\n",
    "#║    'remove 1 Cappuccino'       -> Remove one Cappuccino                          ║#\n",
    "#║    'show my order'             -> See your current order (ask naturally)         ║#\n",
    "#║    'clear my order'            -> Start your order fresh                         ║#\n",
    "#║    'exit' or 'bye'             -> End the session                                ║#\n",
    "#╚════════════════════════════════════════════════════════════════════════════════╝#"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
